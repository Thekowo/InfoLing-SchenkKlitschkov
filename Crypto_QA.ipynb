{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "Crypto QA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thekowo/InfoLing-SchenkKlitschkov/blob/main/Crypto_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcaJc4-87CUI"
      },
      "source": [
        "# Install the latest master of Haystack\n",
        "## You have to restart the runtime after this is complete! The Notebook should alert you of this.\n",
        "!pip install grpcio-tools\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvx2XlgD8A7O"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSHc8ptp8DUy"
      },
      "source": [
        "## This cell grabs all questions from an online article that answers the most googled questions about crypto\n",
        "url = 'https://time.com/nextadvisor/investing/cryptocurrency/most-googled-crypto-questions-answered'\n",
        "page=requests.get(url)\n",
        "soup = BeautifulSoup(page.text)\n",
        "content = soup.body\n",
        "questions=[]\n",
        "for h in content.find_all('h3'):\n",
        "     question=h.text\n",
        "     if('?' in question):\n",
        "        questions.append(h.text)\n",
        "\n",
        "pars=[]\n",
        "for x in content.find_all('p'):\n",
        "     answer=x.text\n",
        "     if((len(answer))>20):\n",
        "      pars.append(answer)\n",
        "all_answers = pars[0:53]\n",
        "\n",
        "f = open(\"doc.txt\", \"w\")\n",
        "for i in range(len(all_answers)):\n",
        "    f.write(all_answers[i] + '\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ_OvE_6ayXL"
      },
      "source": [
        "url = 'https://time.com/nextadvisor/investing/cryptocurrency/most-googled-crypto-questions-answered'\n",
        "page=requests.get(url)\n",
        "soup = BeautifulSoup(page.text)\n",
        "content = soup.body"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x3791kY7CUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e53ec23-095b-4966-f657-35b4db373b24"
      },
      "source": [
        "from haystack.preprocessor.cleaning import clean_wiki_text\n",
        "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
        "from haystack.reader.farm import FARMReader\n",
        "from haystack.reader.transformers import TransformersReader\n",
        "from haystack.utils import print_answers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
            "  \"update your install command.\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7McET0r77CUK"
      },
      "source": [
        "# In Colab / No Docker environments: Start Elasticsearch from source\n",
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "! chown -R daemon:daemon elasticsearch-7.9.2\n",
        "\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                  )\n",
        "# wait until ES has started\n",
        "! sleep 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w6fgqYSf7CUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89d5267-b705-4917-865f-136b6c44cd39"
      },
      "source": [
        "# Connect to Elasticsearch\n",
        "\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")\n",
        "document_store.delete_all_documents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method ElasticsearchDocumentStore.delete_all_documents of <haystack.document_store.elasticsearch.ElasticsearchDocumentStore object at 0x7f2ec899f790>>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FB7MAM-s7CUK"
      },
      "source": [
        "# txt. file ordner in dem angegeben path ablegen!\n",
        "file_path= 'txt_files.zip'\n",
        "path = 'data/crypto'\n",
        "\n",
        "\n",
        "from zipfile import ZipFile\n",
        "with ZipFile(file_path, 'r') as zipObj:\n",
        "  zipObj.extractall(path)\n",
        "import os\n",
        "\n",
        "wrong_path= 'data/crypto/txt_files/~BROMIUM'\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# unzipping the file in collab creates some additional directory called bromium, this should be deleted:\n",
        "# Get directory name\n",
        "mydir= wrong_path\n",
        "#delete the entire bromium directory\n",
        "try:\n",
        "    shutil.rmtree(mydir)\n",
        "except OSError as e:\n",
        "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "\n",
        "docs = path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRoKxBt_Dg6e"
      },
      "source": [
        "## this cell converts the documents and writes them, with light preprocessing \n",
        "\n",
        "## THIS IS THE DEFAULT IMPLEMENTATION\n",
        "\n",
        "dicts = convert_files_to_dicts(dir_path=docs, clean_func=clean_wiki_text, split_paragraphs=True)\n",
        "\n",
        "# writing the dicts to our document store\n",
        "document_store.write_documents(dicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OJogbS2yIye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7474961-7020-4b4c-c9f4-e6588bbca35f"
      },
      "source": [
        "## if splitting is required, this is the cell that splits the documents, and then writes to document store\n",
        "\n",
        "## THIS IS NOT THE DEFAULT IMPLEMENTATION\n",
        "\n",
        "from haystack.preprocessor import PreProcessor\n",
        "processor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=True,\n",
        "    split_by=\"word\",\n",
        "    split_length=300,\n",
        "    split_respect_sentence_boundary=True,\n",
        "    split_overlap=0\n",
        ")\n",
        "\n",
        "\n",
        "# Convert files to dicts, with the already pre defined clean_wiki_text cleaning function\n",
        "dicts = convert_files_to_dicts(dir_path=docs, clean_func=clean_wiki_text, split_paragraphs=True)\n",
        "\n",
        "new_array=[]\n",
        "for i in dicts:\n",
        "  x = processor.process(i)\n",
        "  for j in x:\n",
        "    new_array.append(j)\n",
        "document_store.write_documents(new_array)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "One or more sentence found with word count higher than the split length.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbSLp9CWEU5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc670aa4-3d43-4f12-fde8-005c36769ba9"
      },
      "source": [
        "# Load a model that is suitable for the task - we chose roberta-base-squad2)\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
        "\n",
        "from haystack.retriever.sparse import ElasticsearchRetriever\n",
        "# ExtractiveQAPipeline lets you specify a retriever and a reader, and extracts answers for queries\n",
        "from haystack.pipeline import ExtractiveQAPipeline, GenerativeQAPipeline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/roberta-base-squad2 were not used when initializing RobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LagfTroUdjuw",
        "outputId": "11111fb8-7d77-4753-90b5-f40854da9510"
      },
      "source": [
        "#die trainingsdatei answers.json muss wurde hier in den data ordner eingefügt\n",
        "data_dir = \"data\"\n",
        "reader.train(data_dir=data_dir, train_filename=\"answers.json\", use_gpu=True, n_epochs=1, save_dir=\"my_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing Dataset data/answers.json: 100%|██████████| 1/1 [00:00<00:00,  8.81 Dicts/s]\n",
            "Train epoch 0/0 (Cur. train loss: 0.0026): 100%|██████████| 24/24 [00:21<00:00,  1.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "U3s2gM2N7CUM"
      },
      "source": [
        "## THIS IS THE DEFAULT IMPLEMENTATION\n",
        "\n",
        "## specifies which retriever to use, and establishes the pipe\n",
        "retriever = ElasticsearchRetriever(document_store=document_store)\n",
        "base_pipe = ExtractiveQAPipeline(reader,retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "Pq-20PFg7CUN"
      },
      "source": [
        "## THIS IS THE DEFAULT IMPLEMENTATION\n",
        "\n",
        "# runs all questions through the model, saves them into a txt file\n",
        "# You can configure how many candidates the reader and retriever shall return with the k params\n",
        "# The higher top_k_retriever, the better (but also the slower) your answers. \n",
        "answers =[]\n",
        "for i in range(len(questions)):\n",
        "  answers.append(base_pipe.run(query=questions[i], params= {\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}) ) \n",
        "  \n",
        "f = open(\"answers.txt\", \"w\")\n",
        "for i in range(len(answers)):\n",
        "  f.write('\\n' +questions[i]+ '\\n\\n')\n",
        "  for y in range (len(answers[i]['answers'])):\n",
        "      f.write((answers[i]['answers'][y]['answer'])+'\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvAEpJ5AEMh-"
      },
      "source": [
        "## THIS IS NOT THE DEFAULT IMPLEMENTATION\n",
        "\n",
        "## specifies which retriever to use, and establishes the pipe \n",
        "test_pipe = ExtractiveQAPipeline(reader, test_retriever)\n",
        "test_retriever=ElasticsearchRetriever(document_store=test_store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQxgb0L8yn1q"
      },
      "source": [
        "## THIS IS NOT THE DEFAULT IMPLEMENTATION\n",
        "\n",
        "# You can configure how many candidates the reader and retriever shall return\n",
        "# The higher top_k_retriever, the better (but also the slower) your answers. \n",
        "test_answers =[]\n",
        "for i in range(len(questions)):\n",
        "  test_answers.append(test_pipe.run(query=questions[i], params= {\"Retriever\": {\"top_k\": 15}, \"Reader\": {\"top_k\": 5}}) ) \n",
        "  \n",
        "f = open(\"test.txt\", \"w\")\n",
        "for i in range(len(test_answers)):\n",
        "  f.write('\\n' +questions[i]+ '\\n\\n')\n",
        "  for y in range (len(test_answers[i]['answers'])):\n",
        "      f.write((test_answers[i]['answers'][y]['answer'])+'\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t90mIy8QgHsX"
      },
      "source": [
        "## the following blocks use whatever reader is declared above, but establish a document store with embeddings\n",
        "## and an embedding retriever\n",
        "\n",
        "from haystack.preprocessor.cleaning import clean_wiki_text\n",
        "from haystack.preprocessor.utils import convert_files_to_dicts, fetch_archive_from_http\n",
        "from haystack.reader.farm import FARMReader\n",
        "from haystack.reader.transformers import TransformersReader\n",
        "from haystack.utils import print_answers\n",
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "\n",
        "\n",
        "## this is the document store for dense passage retrieval\n",
        "embed_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
        "embed_store.write_documents(dicts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCvgvsBeAoan"
      },
      "source": [
        "##  initialises the dense passage retriever \n",
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "embed_retriever = DensePassageRetriever(document_store=embed_store,\n",
        "                                  query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "                                  passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
        "                                  max_seq_len_query=64,\n",
        "                                  max_seq_len_passage=256,\n",
        "                                  batch_size=16,\n",
        "                                  use_gpu=True,\n",
        "                                  embed_title=True,\n",
        "                                  use_fast_tokenizers=True)\n",
        "# Updates the embeddings in the document store \n",
        "embed_store.update_embeddings(embed_retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC2K1fL6BC71"
      },
      "source": [
        "## establishes the pipeline including the DPR retriever/embed retriever\n",
        "#reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
        "from haystack.pipeline import ExtractiveQAPipeline\n",
        "pipe = ExtractiveQAPipeline(reader, embed_retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN_uapa8gYLq"
      },
      "source": [
        "embed_answers=[]\n",
        "for i in range(len(questions)):\n",
        "  embed_answers.append(pipe.run(query=questions[i],  params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}))\n",
        "                                          #params= {\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}) ) \n",
        "f = open(\"embed.txt\", \"w\")\n",
        "for i in range(len(embed_answers)):\n",
        "  f.write('\\n' +questions[i]+ '\\n\\n')\n",
        "  for y in range (len(embed_answers[i]['answers'])):\n",
        "      f.write((embed_answers[i]['answers'][y]['answer'])+'\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYROsAY_CPMu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}